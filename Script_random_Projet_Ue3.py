# -*- coding: utf-8 -*-
"""
Random Forest sur donnÃ©es hyperspectrales
Auteur : ChatGPT & [Ton nom]
"""

# ğŸ“¦ Importation des librairies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.tree import plot_tree

# ğŸ“‚ Chargement des donnÃ©es
file_path = r"C:\Users\schwe\Documents\Saves\Python\combined_data.csv"
df = pd.read_csv(file_path)

# ğŸ¯ SÃ©paration des variables
X = df.drop(columns=["class"]).values
y = df["class"].values

# ğŸ”¢ Encodage des classes
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# âœ‚ï¸ SÃ©paration en train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
)

# ğŸŒ² EntraÃ®nement du modÃ¨le Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# ğŸ” PrÃ©dictions et Ã©valuation
y_pred = rf_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"\nğŸ¯ Accuracy du modÃ¨le Random Forest : {accuracy:.2%}\n")
print("ğŸ“Š Rapport de classification :")
print(classification_report(y_test, y_pred, target_names=le.classes_))

# ğŸ“‰ Matrice de confusion
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap="Blues",
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Matrice de confusion â€“ Random Forest")
plt.xlabel("Classe prÃ©dite")
plt.ylabel("Classe rÃ©elle")
plt.tight_layout()
plt.show()

# ğŸ”¬ Importance des variables (top 20)
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]
top_n = 20
top_features = df.columns[:-1].astype(float)[indices[:top_n]]
top_importances = importances[indices[:top_n]]

plt.figure(figsize=(10, 6))
sns.barplot(x=top_importances, y=top_features)
plt.title("Importance des 20 longueurs dâ€™onde les plus discriminantes")
plt.xlabel("Importance")
plt.ylabel("Longueur dâ€™onde (nm)")
plt.tight_layout()
plt.show()

# ğŸ“ˆ Accuracy vs. nombre dâ€™arbres
n_estimators_range = range(10, 210, 10)
cv_scores = []

for n in n_estimators_range:
    model = RandomForestClassifier(n_estimators=n, random_state=42)
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    cv_scores.append(scores.mean())

plt.figure(figsize=(8, 4))
plt.plot(n_estimators_range, cv_scores, marker='o', color='green')
plt.title("Accuracy vs. nombre dâ€™arbres (CV 5-fold)")
plt.xlabel("Nombre dâ€™arbres")
plt.ylabel("Accuracy moyenne")
plt.grid(True)
plt.tight_layout()
plt.show()

# ğŸŒ³ Visualisation de quelques arbres de dÃ©cision
for idx in range(3):  # arbres nÂ°1 Ã  3
    tree = rf_model.estimators_[idx]
    plt.figure(figsize=(20, 10))
    plot_tree(tree,
              feature_names=df.columns[:-1],
              class_names=le.classes_,
              filled=True,
              rounded=True,
              max_depth=3,
              fontsize=8)
    plt.title(f"Arbre de dÃ©cision nÂ°{idx + 1} (profondeur max = 3)")
    plt.tight_layout()
    plt.show()
